#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

"""
 Counts words in text encoded with UTF8 received from the network every second.

 Usage: recoverable_network_wordcount.py <hostname> <port> <checkpoint-directory> <output-file>
   <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive
   data. <checkpoint-directory> directory to HDFS-compatible file system which checkpoint data
   <output-file> file to which the word counts will be appended

 To run this on your local machine, you need to first run a Netcat server
    `$ nc -lk 9999`

 and then run the example
    `$ bin/spark-submit examples/src/main/python/streaming/recoverable_network_wordcount.py \
        localhost 9999 ~/checkpoint/ ~/out`

 If the directory ~/checkpoint/ does not exist (e.g. running for the first time), it will create
 a new StreamingContext (will print "Creating new context" to the console). Otherwise, if
 checkpoint data exists in ~/checkpoint/, then it will create StreamingContext from
 the checkpoint data.
"""
from __future__ import print_function

import os
import sys
#import ast
import itertools

from pyspark import SparkContext
from pyspark.streaming import StreamingContext


def createContext(host, port, outputPath1, outputPath2):
    # If you do not see this printed, that means the StreamingContext has been loaded
    # from the new checkpoint
    print("Creating new context")
    if os.path.exists(outputPath1):
        os.remove(outputPath1)
    sc = SparkContext(appName="PythonStreamingWindowedRecoverableNetworkWordCount")
    ssc = StreamingContext(sc, 1)

    # Create a socket stream on target ip:port and count the
    # words in input stream of \n delimited text (eg. generated by 'nc')
    lines = ssc.socketTextStream(host, port)

    words = lines.flatMap(lambda line: line.split(" ")).filter(lambda x: x != '')

    #windowed word counts
    wordCounts = words.map(lambda x: (x, 1)).reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 30, 10)

    wordCounts.checkpoint(120)

    #returns word counts sorted by count in the form (count, word)
    sortedWordCounts = wordCounts.map(lambda (word, count):(count,word)).transform(lambda rdd: rdd.sortByKey(False))

#    list of the top three words per window with their position as key (ex. [(1,'word1'),(2,'word2')])
#    topThreeWordCountsWithIndex = sortedWordCounts.transform(lambda rdd: rdd.zipWithIndex().filter(lambda (k,v): v<3).map(lambda (k,v):(v,k)))


    def updateFunc(new_values, last_list):
        if last_list is None:
            last_list = []
        counts = "OLD: %s --> NEW: %s" % (last_list, new_values)
        new = "%s" % new_values
        print("updateFunc: "+counts)
        #
        paddedOldRanking =  [i for i,j in list(itertools.izip_longest(last_list, range(10)))]
        diff = [j for i, j in zip(paddedOldRanking, new_values) if i != j]
        if len(diff)>0:
            print("new elements: %s (new ranking: %s)" % (diff, new))
            #write the new ranking on file outputPath2 only when there is a change in any of the the top N positions
            with open(outputPath2, 'a') as f:
                f.write(new + "\n")
        # write the comparison between old and new status on file outputPath1 every time updateFunc is invoked
        with open(outputPath1, 'a') as f:
            f.write(counts + "\n")
        return new_values


#topthree

#    print("topThreeWordCountsWithIndex")
#    topThreeWordCountsWithIndex.pprint()
#    status1 = topThreeWordCountsWithIndex.updateStateByKey(updateFunc)
#    status1.pprint()
    #.map(lambda (k, (count,word)):(k,word))

    #for each time window, create a unique ranking RDD having the format ('ranking', [SORTED,LIST, OF, WORDS]) that will be used as state
    rankingWordCount = sortedWordCounts.transform(lambda rdd:rdd.map(lambda (k,v):('ranking',v)).groupByKey().map(lambda x : (x[0], list(x[1]))))
    rankingWordCount.pprint()
    #invoke updateFunc for each time window
    status = rankingWordCount.updateStateByKey(updateFunc)
    status.pprint()
    return ssc


if __name__ == "__main__":
    if len(sys.argv) != 6:
        print("Usage: recoverable_network_wordcount.py <hostname> <port> "
              "<checkpoint-directory> <output-file1> <output-file2>", file=sys.stderr)
        exit(-1)
    ranking = [1,2,3]
    host, port, checkpoint, output1, output2 = sys.argv[1:]
    ssc = StreamingContext.getOrCreate(checkpoint,
                                       lambda: createContext(host, int(port), output1, output2))
    ssc.start()
    ssc.awaitTermination()
